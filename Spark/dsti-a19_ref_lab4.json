{"paragraphs":[{"text":"%md\nHow to submit the Spark application:\n1. SSH to the edge server\n2. Get the code from HDFS\n   ```\n   hdfs dfs -get /learning/code/spark .\n   ```\n3. Cd to the code folder: `cd spark/application-submitting`\n4. Check out the documentation at [https://github.com/adaltas/ece-spark/tree/master/application-submitting](https://github.com/adaltas/ece-spark/tree/master/application-submitting)","user":"gauthier","dateUpdated":"2020-03-18T15:24:16+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>How to submit the Spark application:</p>\n<ol>\n<li>SSH to the edge server</li>\n<li>Get the code from HDFS<pre><code>hdfs dfs -get /learning/code/spark .\n</code></pre>\n</li>\n<li>Cd to the code folder: <code>cd spark/application-submitting</code></li>\n<li>Check out the documentation at <a href=\"https://github.com/adaltas/ece-spark/tree/master/application-submitting\">https://github.com/adaltas/ece-spark/tree/master/application-submitting</a></li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1584541249832_-179617475","id":"20200318-142049_1953575637","dateCreated":"2020-03-18T14:20:49+0000","dateStarted":"2020-03-18T15:24:16+0000","dateFinished":"2020-03-18T15:24:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6503"},{"text":"%md\n```py\nimport argparse\nimport getpass\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split, window, count, mean\n\n# Use Python argparse library to parse arguments\nparser = argparse.ArgumentParser(\n    description='Process NYC Taxi datasets in streaming using sockets')\nparser.add_argument('appname', type=str, help='The Spark application name')\nparser.add_argument('sockethostname', type=str,\n                    help='The hostname on which to listen to the socket')\nparser.add_argument('outputpath', type=str,\n                    help='The HDFS path where to write the CSV output')\nparser.add_argument('-f', '--faresport', type=int, default=11111,\n                    help='The port on which the fares dataset is streamed')\nparser.add_argument('-r', '--ridesport', type=int, default=11112,\n                    help='The port on which the rides dataset is streamed')\nparser.add_argument('-c', '--checkpoint', type=str, help='The HDFS path '\n                    'where Spark will write checkpointing infomation. '\n                    'Default = /user/USERNAME/checkpoint/APP_NAME')\n\nargs = parser.parse_args()\n\n# Create a new SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(args.appname) \\\n    .getOrCreate()\n\n# Print the application Web UI url\nprint('Application Web UI: %s' % spark.sparkContext.uiWebUrl)\n\n# Define the stream source (socket)\nfares_raw = spark \\\n    .readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", args.sockethostname) \\\n    .option(\"port\", args.faresport) \\\n    .load()\n\n# Parse the socket message \"manually\"\nfares = fares_raw \\\n    .select(\n        split(fares_raw.value, ',')[0].alias('ride_id').cast('int'),\n        split(fares_raw.value, ',')[1].alias('taxi_id').cast('int'),\n        split(fares_raw.value, ',')[2].alias('driver_id').cast('int'),\n        split(fares_raw.value, ',')[3].alias('start_time').cast('timestamp'),\n        split(fares_raw.value, ',')[4].alias('payment_type'),\n        split(fares_raw.value, ',')[5].alias('tip').cast('float'),\n        split(fares_raw.value, ',')[6].alias('tolls').cast('float'),\n        split(fares_raw.value, ',')[7].alias('total_fare').cast('float')\n    ) \\\n    .withWatermark('start_time', '1 minutes') \\\n\n# Define the aggregation to perform on the stream\nfares_count = fares \\\n    .withWatermark('start_time', '1 minutes') \\\n    .groupBy(window(fares.start_time, '1 minutes', '1 minutes')) \\\n    .agg(\n        count('ride_id').alias('ride_count'),\n        mean('total_fare').alias('mean_total_fare')\n    )\n\n# Define the stream sick (parquet files written to HDFS)\n# The trigger control the occurance of parquet file generation\nfares_count_query = fares_count \\\n    .writeStream \\\n    .outputMode('append') \\\n    .format('parquet') \\\n    .trigger(processingTime='10 seconds') \\\n    .option('path', args.outputpath) \\\n    .option('checkpointLocation', args.checkpoint if args.checkpoint\n            else '/user/{}/checkpoint/{}'.format(\n                getpass.getuser(), args.appname)) \\\n    .start()\n\n# Wait for the end of the query before ending the application\nfares_count_query.awaitTermination()\n\n```","user":"gauthier","dateUpdated":"2020-03-18T14:26:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"py\">import argparse\nimport getpass\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split, window, count, mean\n\n# Use Python argparse library to parse arguments\nparser = argparse.ArgumentParser(\n    description='Process NYC Taxi datasets in streaming using sockets')\nparser.add_argument('appname', type=str, help='The Spark application name')\nparser.add_argument('sockethostname', type=str,\n                    help='The hostname on which to listen to the socket')\nparser.add_argument('outputpath', type=str,\n                    help='The HDFS path where to write the CSV output')\nparser.add_argument('-f', '--faresport', type=int, default=11111,\n                    help='The port on which the fares dataset is streamed')\nparser.add_argument('-r', '--ridesport', type=int, default=11112,\n                    help='The port on which the rides dataset is streamed')\nparser.add_argument('-c', '--checkpoint', type=str, help='The HDFS path '\n                    'where Spark will write checkpointing infomation. '\n                    'Default = /user/USERNAME/checkpoint/APP_NAME')\n\nargs = parser.parse_args()\n\n# Create a new SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(args.appname) \\\n    .getOrCreate()\n\n# Print the application Web UI url\nprint('Application Web UI: %s' % spark.sparkContext.uiWebUrl)\n\n# Define the stream source (socket)\nfares_raw = spark \\\n    .readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", args.sockethostname) \\\n    .option(\"port\", args.faresport) \\\n    .load()\n\n# Parse the socket message \"manually\"\nfares = fares_raw \\\n    .select(\n        split(fares_raw.value, ',')[0].alias('ride_id').cast('int'),\n        split(fares_raw.value, ',')[1].alias('taxi_id').cast('int'),\n        split(fares_raw.value, ',')[2].alias('driver_id').cast('int'),\n        split(fares_raw.value, ',')[3].alias('start_time').cast('timestamp'),\n        split(fares_raw.value, ',')[4].alias('payment_type'),\n        split(fares_raw.value, ',')[5].alias('tip').cast('float'),\n        split(fares_raw.value, ',')[6].alias('tolls').cast('float'),\n        split(fares_raw.value, ',')[7].alias('total_fare').cast('float')\n    ) \\\n    .withWatermark('start_time', '1 minutes') \\\n\n# Define the aggregation to perform on the stream\nfares_count = fares \\\n    .withWatermark('start_time', '1 minutes') \\\n    .groupBy(window(fares.start_time, '1 minutes', '1 minutes')) \\\n    .agg(\n        count('ride_id').alias('ride_count'),\n        mean('total_fare').alias('mean_total_fare')\n    )\n\n# Define the stream sick (parquet files written to HDFS)\n# The trigger control the occurance of parquet file generation\nfares_count_query = fares_count \\\n    .writeStream \\\n    .outputMode('append') \\\n    .format('parquet') \\\n    .trigger(processingTime='10 seconds') \\\n    .option('path', args.outputpath) \\\n    .option('checkpointLocation', args.checkpoint if args.checkpoint\n            else '/user/{}/checkpoint/{}'.format(\n                getpass.getuser(), args.appname)) \\\n    .start()\n\n# Wait for the end of the query before ending the application\nfares_count_query.awaitTermination()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1584541348855_-1336393434","id":"20200318-142228_1925081654","dateCreated":"2020-03-18T14:22:28+0000","dateStarted":"2020-03-18T14:26:58+0000","dateFinished":"2020-03-18T14:26:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6504"},{"text":"%pyspark\nnyxtaxi_app_result = spark.read.parquet('/user/gauthier/spark-streaming-demo-dsti')\nz.show(nyxtaxi_app_result)","user":"gauthier","dateUpdated":"2020-03-18T14:42:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"window":"string","ride_count":"string","mean_total_fare":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"window\tride_count\tmean_total_fare\n[2020-03-18 14:34:00.0,2020-03-18 14:35:00.0]\t92\t14.865869532460751\n[2020-03-18 14:35:00.0,2020-03-18 14:36:00.0]\t115\t13.6879999741264\n[2020-03-18 14:36:00.0,2020-03-18 14:37:00.0]\t143\t15.448951027610086\n[2020-03-18 14:37:00.0,2020-03-18 14:38:00.0]\t186\t15.338924766868674\n[2020-03-18 14:38:00.0,2020-03-18 14:39:00.0]\t199\t13.32271354761555\n[2020-03-18 14:39:00.0,2020-03-18 14:40:00.0]\t259\t14.080772213954262\n[2020-03-18 14:40:00.0,2020-03-18 14:41:00.0]\t337\t14.773115718400089\n"},{"type":"TEXT","data":"\n"}]},"runtimeInfos":{},"apps":[],"jobName":"paragraph_1584541570955_-1423465017","id":"20200318-142610_766764622","dateCreated":"2020-03-18T14:26:10+0000","dateStarted":"2020-03-18T14:42:28+0000","dateFinished":"2020-03-18T14:42:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6505"},{"text":"%jdbc\n","user":"gauthier","dateUpdated":"2020-03-18T14:37:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584542254336_57958182","id":"20200318-143734_1529411381","dateCreated":"2020-03-18T14:37:34+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6506"}],"name":"dsti-a19/ref/lab4","id":"2F5RMQCRC","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"spark2:gauthier:":[],"jdbc:gauthier:":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}